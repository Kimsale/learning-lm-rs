mod config;
mod kvcache;
mod model;
mod operators;
mod params;
mod tensor;
mod chat;

use std::path::{PathBuf, Path};
use eframe::egui;
use tokenizers::Tokenizer;

#[derive(Default)]
struct App {
    // ç•Œé¢çŠ¶æ€
    model_type: String,
    input_text: String,
    output_text: String,
    is_processing: bool,
    
    // æ¨¡å‹å®ä¾‹
    llama_chat: Option<model::Llama<f32>>,
    llama_story: Option<model::Llama<f32>>,

    // å…±äº«èµ„æº
    tokenizer: Option<Tokenizer>,
    cache: Option<kvcache::KVCache<f32>>,

    start_time: Option<std::time::Instant>,  
    elapsed_secs: f32,                       
}

impl eframe::App for App {
    fn update(&mut self, ctx: &egui::Context, _frame: &mut eframe::Frame) {
        egui::SidePanel::left("control_panel").show(ctx, |ui| {
            ui.heading("Model Control");
            ui.separator();
            
            // æ¨¡å¼é€‰æ‹©
            egui::ComboBox::from_label("Operation Mode")
                .selected_text(&self.model_type)
                .show_ui(ui, |ui| {
                    ui.selectable_value(&mut self.model_type, "chat".into(), "Conversational Mode");
                    ui.selectable_value(&mut self.model_type, "story".into(), "Continuation mode");
                });
            
            ui.separator();
            
            // æ¨¡å‹åŠ è½½æŒ‰é’®
            if ui.button("Loading the model").clicked() {
                self.load_model();
            }
            
            ui.separator();
            ui.label("State:");
            if self.llama_chat.is_some() || self.llama_story.is_some() {
                ui.label(" Model loaded");
            } else {
                ui.label(" Model not loaded");
            }
        });

        egui::CentralPanel::default().show(ctx, |ui| {
            ui.heading("Text Generation");
            ui.separator();
            
            // è¾“å…¥åŒºåŸŸ
            ui.label("Input text:");
            ui.text_edit_multiline(&mut self.input_text);
            
            // ç”ŸæˆæŒ‰é’®
            ui.horizontal(|ui| {
                if ui.button("Generating text").clicked() && !self.is_processing {
                    self.start_generation();
                }
                if self.is_processing {
                    ui.spinner();
                }
            });

            // ç”Ÿæˆå®Œæˆåè°ƒç”¨ï¼ˆä¾‹å¦‚åœ¨ç”Ÿæˆçº¿ç¨‹ç»“æŸæ—¶ï¼‰
            self.finalize_generation();
            
            ui.separator();
            
            // è¾“å‡ºåŒºåŸŸ
            ui.label("Generate results:");
            ui.text_edit_multiline(&mut self.output_text);

            // åœ¨è¾“å‡ºåŒºåŸŸä¸‹æ–¹æ·»åŠ 
            ui.separator();
            ui.label(format!("ğŸ•’ This generation took: {:.1} seconds", self.elapsed_secs));
        });
    }
}

impl App {
    fn load_model(&mut self) {
        let model_path = build_model_path(&self.model_type);
        
        // åŠ è½½åˆ†è¯å™¨
        self.tokenizer = Tokenizer::from_file(model_path.join("tokenizer.json"))
            .ok();
        
        // åŠ è½½æ¨¡å‹
        match self.model_type.as_str() {
            "chat" => {
                self.llama_chat = Some(model::Llama::from_safetensors(model_path));
            }
            "story" => {
                self.llama_story = Some(model::Llama::from_safetensors(model_path));
            }
            _ => {}
        }
    }

    fn start_generation(&mut self) {
        if self.tokenizer.is_none() {
            self.output_text = "Please load the model first".to_string();
            return;
        }

        // è®°å½•å¼€å§‹æ—¶é—´
        self.start_time = Some(std::time::Instant::now());
        let input = self.input_text.clone();
        let tokenizer = self.tokenizer.as_ref().unwrap();
        let mut cache = self.cache.take().unwrap_or_else(|| {
            match self.model_type.as_str() {
                "chat" => self.llama_chat.as_ref().unwrap().new_cache(),
                _ => self.llama_story.as_ref().unwrap().new_cache(),
            }
        });

        // æ‰§è¡Œç”Ÿæˆ
        let binding = tokenizer.encode(&*input, true).unwrap();
        let input_ids = binding.get_ids();
        
        let output_ids = match self.model_type.as_str() {
            "chat" => self.llama_chat.as_ref().unwrap().generate_cache(
                input_ids, 250, 0.8, 30, 0.7, &mut cache
            ),
            _ => self.llama_story.as_ref().unwrap().generate(
                input_ids, 500, 0.8, 30, 0.7
            ),
        };
        
        // ä¿å­˜ç¼“å­˜
        self.cache = Some(cache);
        
        // è§£ç ç»“æœ
        self.output_text = tokenizer.decode(&output_ids, true)
            .unwrap()
            .replace("\n<|im_end|>\n", "")
            .trim()
            .to_string();

    }

    // åœ¨ç”Ÿæˆå®Œæˆåè°ƒç”¨
    fn finalize_generation(&mut self) {
        // è®¡ç®—è€—æ—¶
        if let Some(start) = self.start_time {
            self.elapsed_secs = start.elapsed().as_secs_f32();
        }
    }
}

fn build_model_path(model_type: &str) -> PathBuf {
    PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .join("models")
        .join(model_type)
}

fn main() -> eframe::Result<()> {
    let options = eframe::NativeOptions {
        // ä¿®æ­£çª—å£å¤§å°è®¾ç½®
        viewport: egui::ViewportBuilder::default()
            .with_inner_size([800.0, 600.0]),
        ..Default::default()
    };
    
    eframe::run_native(
        "UI PLATFORM", 
        options, 
        //Box::new(|_cc| Box::new(App::default()))
        Box::new(|_cc| Ok(Box::new(App::default())))
    )
}


/// ä»¥ä¸‹æ˜¯æ— UIçš„å®ç°

// mod config;
// mod kvcache;
// mod model;
// mod operators;
// mod params;
// mod tensor;
// mod chat;

// use std::path::{Path, PathBuf};
// use clap::Parser;
// use tokenizers::Tokenizer;
// use crate::model::Llama;
// use crate::kvcache::KVCache;


// /* fn main() {
//     let project_dir = env!("CARGO_MANIFEST_DIR");
//     let model_dir = PathBuf::from(project_dir).join("models").join("story");
//     let llama = model::Llama::<f32>::from_safetensors(&model_dir);
//     let tokenizer = Tokenizer::from_file(model_dir.join("tokenizer.json")).unwrap();
    
//     let input = "Once upon a time";
//     let binding = tokenizer.encode(input, true).unwrap();
//     let input_ids = binding.get_ids();

//     //print!("\n{}\n", input);
//     let output_ids = llama.generate(
//         input_ids,
//         500,  //max_len
//         0.8,  //top_p
//         30,   //top_k
//         0.7,   //temperature
//     );
//     println!("{}", tokenizer.decode(&output_ids, true).unwrap());
// } */



// #[derive(Parser, Debug)]
// #[command(version, about)]
// struct Args {
//     #[arg(short, long)]
//     mode: String,

//     #[arg(long, default_value_t = 500)]
//     max_len: usize,

//     #[arg(long, default_value_t = 0.8)]
//     top_p: f32,

//     #[arg(long, default_value_t = 30)]
//     top_k: u32,

//     #[arg(long, default_value_t = 0.7)]
//     temperature: f32,
// }

// fn main() {
//     let args = Args::parse();
    
//     match args.mode.as_str() {
//         "chat" => run_chat_mode(),
//         _ => run_story_mode(&args),
//     }
//     //generate_chat()
// }

// fn generate_chat(    
//     input: String,
//     tokenizer: &Tokenizer,  // æ”¹ä¸ºå¼•ç”¨
//     llama: &Llama<f32>,     // æ”¹ä¸ºå¼•ç”¨
//     cache: &mut KVCache<f32> // æ”¹ä¸ºå¯å˜å¼•ç”¨
//     ) -> String {
//     // let mut input = String::new();
//     // std::io::stdin().read_line(&mut input).unwrap();
//     // let user_input = input.trim();
    
//     println!("start generate_chat");
//     // ç¼–ç è¾“å…¥
//     // let binding = tokenizer.encode(dialog_history.as_str(), true).unwrap();
//     let binding = tokenizer.encode(input, true).unwrap();
//     let input_ids = binding.get_ids();
    
//     // ç”Ÿæˆå›å¤
//     println!("go to generate_cache");
//     let result = llama.generate_cache(
//         input_ids,
//         250,  // æ¯è½®æœ€å¤§ç”Ÿæˆé•¿åº¦
//         0.8,
//         30,
//         0.7,
//         cache
//     );
//     println!("finish generate_cache");
    
//     // è§£ç å¹¶æ›´æ–°å†å²
//     let response = tokenizer.decode(&result, true).unwrap();
//     const END_MARKER: &str = "\n<|im_end|>\n";
//     let clean_response = response.replace(END_MARKER, "").trim().to_string();
//     // println!("Assistant: {}", clean_response);
//     // println!("Assistant_result: {}", clean_response);
    
//     // æ›´æ–°å¯¹è¯å†å²
//     // dialog_history.push_str(&format!("{}{}", clean_response, END_MARKER));
    
//     // // ç¼“å­˜ç®¡ç†ï¼ˆé™åˆ¶å†å²é•¿åº¦ï¼‰
//     // if dialog_history.len() > 4000 {
//     //     dialog_history.drain(0..2000);
//     //     cache = llama.new_cache(); // å†å²è¿‡é•¿æ—¶é‡ç½®ç¼“å­˜
//     // }

//     let output_text = clean_response;
//     output_text
//     //println!("Assistant_result: {}", output_text);

// }

// // å¯¹è¯æ¨¡å¼å®ç°
// fn run_chat_mode() {
//     let model_dir = build_model_path("chat");
//     //let llama = Llama::from_safetensors(&model_dir);
//     let llama = Llama::from_safetensors(&model_dir);
//     let tokenizer = Tokenizer::from_file(model_dir.join("tokenizer.json")).unwrap();

//     let mut cache = llama.new_cache();

//     // å¯¹è¯æ¨¡æ¿å¸¸é‡
//     const SYSTEM_PROMPT: &str = "<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n";
//     const USER_PREFIX: &str = "<|im_start|>user\n";
//     const ASSISTANT_PREFIX: &str = "<|im_start|>assistant\n";
//     const END_MARKER: &str = "\n<|im_end|>\n";

//     println!("è¿›å…¥å¯¹è¯æ¨¡å¼(è¾“å…¥exité€€å‡º):");
//     loop {
//         let mut input = String::new();
//         std::io::stdin().read_line(&mut input).unwrap();
//         let user_input = input.trim();
        
//         if user_input == "exit" {
//             break;
//         }

//         let input_format = format!("{}{}{}{}{}", SYSTEM_PROMPT, USER_PREFIX, user_input, END_MARKER, ASSISTANT_PREFIX);
//         println!("User:{}",user_input);

//         let response = generate_chat(input_format, &tokenizer, &llama, &mut cache);
//         println!("Assistant: {}", response);
//     }
// }

// // æ–‡æœ¬ç»­å†™æ¨¡å¼å®ç°
// fn run_story_mode(args: &Args) {
//     let model_dir = build_model_path("story");
//     let llama = model::Llama::<f32>::from_safetensors(&model_dir);
//     let tokenizer = Tokenizer::from_file(model_dir.join("tokenizer.json"))
//     .unwrap_or_else(|_| panic!("Storyæ¨¡å‹åˆ†è¯å™¨åŠ è½½å¤±è´¥"));

//     // let binding = tokenizer.encode(&args.input, true).unwrap();

//     println!("è¿›å…¥ç»­å†™æ¨¡å¼ï¼Œè¯·è¾“å…¥è¦ç»­å†™çš„æ–‡æœ¬:");
//     let mut input = String::new();
//     std::io::stdin().read_line(&mut input).unwrap();
//     let input_text = input.trim();

//     //let input_text: &str = &args.input.as_str();
//     let binding = tokenizer.encode(input_text, true).unwrap();

//     let input_ids = binding.get_ids();

//     let output_ids = llama.generate(
//         input_ids,
//         args.max_len,
//         args.top_p,
//         args.top_k,
//         args.temperature,
//     );

//     println!("ç”Ÿæˆç»“æœ:\n{}", tokenizer.decode(&output_ids, true).unwrap());
// }

// // å…¬å…±å·¥å…·å‡½æ•°
// fn build_model_path(mode: &str) -> PathBuf {
//     PathBuf::from(env!("CARGO_MANIFEST_DIR"))
//         .join("models")
//         .join(mode)
// }
